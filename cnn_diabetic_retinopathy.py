# -*- coding: utf-8 -*-
"""cnn diabetic retinopathy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LwtdoYI8aTeNiCNQi9Fvif0Xm2BGg49H
"""

import os
import glob
import numpy as np
from skimage.io import imread
import tensorflow as tf
from tqdm import tqdm_notebook as tqdm
import pandas as pd
import shutil
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

src = '/content/drive/MyDrive/preprocessed/images3'

img_paths = glob.glob(os.path.join(src, '*/*.jpg')) #added / because more folders
parent_names = [os.path.basename(os.path.abspath(os.path.join(p, os.pardir))) for p in img_paths]
#for each path, gets the folder the image is in

len(img_paths)

imgs = np.asarray([imread(p) for p in img_paths])
#turning them into arrays

#conditions = ['normal fundus', 'mild nonproliferative retinopathy', 'moderate non proliferative retinopathy']
#for index, row in data.iterrows():
  #try:
    #if row['Left-Diagnostic Keywords'] in conditions:
     #shutil.copy(src +'/'+ row['Left-Fundus'], '/content/drive/MyDrive/preprocessed/images/' + row['Left-Diagnostic Keywords'])
  #except:
    #print(row['Left-Fundus'])
  #try:
    #if row['Right-Diagnostic Keywords'] in conditions:
     #shutil.copy(src +'/'+ row['Right-Fundus'], '/content/drive/MyDrive/preprocessed/images/' + row['Right-Diagnostic Keywords'])
  #except:
    #print(row['Right-Fundus'])

labels = np.asarray([0 if p == "normal fundus" else 1 if p == "mild nonproliferative retinopathy" else 2 for p in parent_names])
x_train, x_test, y_train, y_test = train_test_split(imgs, labels, test_size = 0.2, random_state = 1) #80:20 
# turns labels into numbers, images and labels split into train, test 
#random state actually doing random is very costly so this is a predetermined "random" (pattern) random_state = 1 the train and test will be the same even if you run it twice

len(labels)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1/255, validation_split = 0.1)
#Look up documentation. Look at creating new data using rotations
test_datagen = ImageDataGenerator(rescale=1/255)

y_train

y_train = tf.keras.utils.to_categorical(y_train, 3)
y_test = tf.keras.utils.to_categorical(y_test, 3)

train_datagen.fit(x_train)
test_datagen.fit(x_test)
#puts all images into datagenerator

train_gen = train_datagen.flow(x_train, y_train, batch_size = 32, subset = 'training') #<--
valid_gen = train_datagen.flow(x_train, y_train, batch_size = 32, subset = 'validation')
test_gen = test_datagen.flow(x= x_test, y = y_test,batch_size = 32) #<--
#matches images with labels (separates them)

pool_size = (4,4) #simplifies image (show details)
kernal_size = (2,2)

x_in = tf.keras.layers.Input(shape=(512, 512, 3))

x = tf.keras.layers.Conv2D(filters=16, kernel_size=kernal_size, activation='relu')(x_in) #16 filters = 16 different kernels, filters are pre generated
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.MaxPool2D(pool_size=pool_size)(x)
# as the data gets compressed, you need more filters to see smaller details 
x = tf.keras.layers.Conv2D(filters=32, kernel_size=kernal_size, activation='relu')(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.MaxPool2D(pool_size=pool_size)(x)

x = tf.keras.layers.Conv2D(filters=64, kernel_size=kernal_size, activation='relu')(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.MaxPool2D(pool_size=pool_size)(x)

x = tf.keras.layers.Conv2D(filters=128, kernel_size=kernal_size, activation='relu')(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.MaxPool2D(pool_size=pool_size)(x)

x_flatten = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(64, activation='relu')(x_flatten) #if you add more nodes, theoritically it picks up more detials

x_out = tf.keras.layers.Dense(3, activation='softmax')(x)
#accuracy vs time

model = tf.keras.Model(x_in, x_out)

model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])

model.summary()

model.fit_generator(train_gen, steps_per_epoch=len(train_gen), epochs=50, validation_data=valid_gen, validation_steps=len(valid_gen)) #fitting to generated images

#overfitting
#ddecrease epochs
#to increase accuracy, you can fine-tune your parameters: eg changing your kernel size, pooling size, etc.

model.evaluate(test_gen, verbose = 1)

